# 快速开始

安装快速而直接。

```
 # airflow needs a home, ~/airflow is the default,
# but you can lay foundation somewhere else if you prefer
# (optional)
export AIRFLOW_HOME = ~/airflow

# install from pypi using pip
pip install apache-airflow

# initialize the database
airflow initdb

# start the web server, default port is 8080
airflow webserver -p 8080

# start the scheduler
airflow scheduler

# visit localhost:8080 in the browser and enable the example dag in the home page

```

运行这些命令后，Airflow将创建`$AIRFLOW_HOME`文件夹，并打下一个“airflow.cfg”文件，其默认值可以让您快速上手。 您可以在`$AIRFLOW_HOME/airflow.cfg`检查文件，也可以通过`$AIRFLOW_HOME/airflow.cfg` `Admin-&gt;Configuration`菜单中的UI检查文件。 如果由systemd启动，则Web服务器的PID文件将存储在`$AIRFLOW_HOME/airflow-webserver.pid`或`/run/airflow/webserver.pid` 。

开箱即用，Airflow使用sqlite数据库，由于使用此数据库后端无法进行并行化，因此您应该很快就会长大。 它与`SequentialExecutor`一起使用，它只能按顺序运行任务实例。 虽然这是非常有限的，但它允许您快速启动和运行并浏览UI和命令行实用程序。

以下是一些将触发一些任务实例的命令。 在运行以下命令时，您应该能够在`example1` DAG中看到作业的状态发生变化。

```
 # run your first task instance
airflow run example_bash_operator runme_0 2015 -01-01
# run a backfill over 2 days
airflow backfill example_bash_operator -s 2015 -01-01 -e 2015 -01-02

```

## 下一步是什么？

从这一点开始，您可以前往“ [教程”](tutorial.html)部分获取更多示例，或者如果您已准备好弄清楚，请参阅[“操作指南”](howto/index.html)部分。